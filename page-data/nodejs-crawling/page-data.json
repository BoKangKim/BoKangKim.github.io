{
    "componentChunkName": "component---src-templates-markdown-layout-tsx",
    "path": "/nodejs-crawling/",
    "result": {"data":{"markdownRemark":{"html":"<p><img src=\"https://user-images.githubusercontent.com/71566740/131489009-db9d044d-b05e-4c6b-b322-7076d5d70cab.png\" alt=\"Robot\"></p>\n<br>\n<h2>크롤링(Crawling)</h2>\n<p>크롤링은 웹 페이지에서 원하는 데이터를 추출해 내는 행위입니다.\r\n<br>크롤링을 위해 개발된 소프트웨어를 크롤러(Crawler)라 합니다.</p>\n<p>저는 보통 API를 만들거나 Tensorflow 학습 데이터를 수집하는데 크롤링을 자주 이용합니다.\r\n<br>크롤링에 활용 가능한 도구는 언어별로 Jsoup(Java), BeautifulSoup(Python) 등 여러 종류가 있지만 이번 포스트에서는 제가 가장 자주 사용하는 Javascript, Node.js 도구들을 이용하겠습니다.\r\n<br>(언어마다 도구는 달라도 동작 방식은 대체로 비슷합니다.)</p>\n<h2><svg role=\"img\" viewBox=\"0 0 24 24\" xmlns=\"http://www.w3.org/2000/svg\"><title>Node.js</title><path d=\"M11.998,24c-0.321,0-0.641-0.084-0.922-0.247l-2.936-1.737c-0.438-0.245-0.224-0.332-0.08-0.383 c0.585-0.203,0.703-0.25,1.328-0.604c0.065-0.037,0.151-0.023,0.218,0.017l2.256,1.339c0.082,0.045,0.197,0.045,0.272,0l8.795-5.076 c0.082-0.047,0.134-0.141,0.134-0.238V6.921c0-0.099-0.053-0.192-0.137-0.242l-8.791-5.072c-0.081-0.047-0.189-0.047-0.271,0 L3.075,6.68C2.99,6.729,2.936,6.825,2.936,6.921v10.15c0,0.097,0.054,0.189,0.139,0.235l2.409,1.392 c1.307,0.654,2.108-0.116,2.108-0.89V7.787c0-0.142,0.114-0.253,0.256-0.253h1.115c0.139,0,0.255,0.112,0.255,0.253v10.021 c0,1.745-0.95,2.745-2.604,2.745c-0.508,0-0.909,0-2.026-0.551L2.28,18.675c-0.57-0.329-0.922-0.945-0.922-1.604V6.921 c0-0.659,0.353-1.275,0.922-1.603l8.795-5.082c0.557-0.315,1.296-0.315,1.848,0l8.794,5.082c0.57,0.329,0.924,0.944,0.924,1.603 v10.15c0,0.659-0.354,1.273-0.924,1.604l-8.794,5.078C12.643,23.916,12.324,24,11.998,24z M19.099,13.993 c0-1.9-1.284-2.406-3.987-2.763c-2.731-0.361-3.009-0.548-3.009-1.187c0-0.528,0.235-1.233,2.258-1.233 c1.807,0,2.473,0.389,2.747,1.607c0.024,0.115,0.129,0.199,0.247,0.199h1.141c0.071,0,0.138-0.031,0.186-0.081 c0.048-0.054,0.074-0.123,0.067-0.196c-0.177-2.098-1.571-3.076-4.388-3.076c-2.508,0-4.004,1.058-4.004,2.833 c0,1.925,1.488,2.457,3.895,2.695c2.88,0.282,3.103,0.703,3.103,1.269c0,0.983-0.789,1.402-2.642,1.402 c-2.327,0-2.839-0.584-3.011-1.742c-0.02-0.124-0.126-0.215-0.253-0.215h-1.137c-0.141,0-0.254,0.112-0.254,0.253 c0,1.482,0.806,3.248,4.655,3.248C17.501,17.007,19.099,15.91,19.099,13.993z\"/></svg>도구 선택</h2>\n<p>node.js에서도 크롤링에 사용할 수 있는 도구도 종류가 많고 그중에 용도에 맞는 도구를 선택하면 되겠습니다.\r\n<br>이 포스트에서는 <a href=\"http://openinsider.com/insider-purchases-25k\">OpenInsider</a>(해외 내부자 거래 정보 사이트)를 크롤링 해보겠습니다.\r\n<br>해당 페이지는 로그인도 필요 없고 따로 크롤링이 차단되어 있지도 않기 때문에 단순 http 라이브러리와 parsing 라이브러리만 사용하겠습니다.</p>\n<p>만약 특정 이유로 사람이 직접 데이터를 수집하는 것처럼 브라우저를 핸들링하는 방법으로 크롤링 해야 한다면 속도는 느리지만 Chromium을 제어하는 도구들(Puppeteer 등)을 사용하시면 됩니다.\r\n<br>(시간이 된다면 Puppeteer 사용방법도 다뤄보도록 하겠습니다.)</p>\n<ul>\n<li>HTTP 라이브러리: Axios\n<ul>\n<li>http 라이브러리에는 종류가 굉장히 많고 저는 평소 Request를 자주 사용해 왔는데 해당 라이브러리가 deprecated 되었다는 소식을 듣고 이번에는 가장 성능이 좋다는 Axios를 사용해 보기로 했습니다.</li>\n</ul>\n</li>\n<li>Parsing 라이브러리: Cheerio\n<ul>\n<li>사실 parsing 라이브러리는 없어도 직접 파싱 해서 사용할 수 있지만 방대한 량의 html 코드를 파싱 하는 과정이 복잡해질뿐더러 코드의 가독성도 떨어집니다.\r\n<br>저는 여기서 jQuery 문법을 그대로 사용할 수 있는 Cheerio를 사용해서 파싱 하겠습니다.</li>\n</ul>\n</li>\n</ul>\n<br>\n<h2>개발환경 설정</h2>\n<h3>새 프로젝트 생성</h3>\n<deckgo-highlight-code language=\"bash\"  >\n          <code slot=\"code\">$ mkdir &lt;프로젝트 이름&gt;\r\n$ cd &lt;프로젝트 이름&gt;\r\n$ npm init</code>\n        </deckgo-highlight-code>\n<h3>사용할 node.js 모듈 설치</h3>\n<deckgo-highlight-code language=\"bash\"  >\n          <code slot=\"code\">npm install axios cheerio</code>\n        </deckgo-highlight-code>\n<h3>크롤링 맛보기</h3>\n<p>이제 원하는 정보들의 위치를 찾아야 합니다.\r\n<br>원하는 정보를 오른쪽 마우스로 클릭후 검사를 사용하면 쉽게 찾을 수 있습니다.\r\n<br>저는 거래 날짜와 해당 주식의 ticker 값을 받아오기 위한 selector를 복사해보겠습니다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/71566740/131478329-82d599e6-56fa-44df-b80a-e609896315f8.png\" alt=\"capture\">\r\n<br>\r\n첫번째 값을 기준으로 각 정보의 slector는 다음과 같은걸 확인 할 수 있습니다.</p>\n<p>거래날짜: <code>#tablewrapper > table > tbody > tr:nth-child(1) > td:nth-child(3) > div</code>\r\n<br>ticker: <code>#tablewrapper > table > tbody > tr:nth-child(1) > td:nth-child(4) > b > a</code>\r\n<br>이 구조를 보면 한 가지 거래 정보는 같은 tr에 포함되어 있다는 걸 알 수 있습니다.</p>\n<br>\n<h3>코드 작성</h3>\n<p>생성한 프로젝트에 index.js 파일을 만들고 코드를 작성합니다.\r\n<br>여러 가지 방법이 있겠지만 저는 map 메서드를 이용해 모든 tr들의 거래 날짜와 정보를 가진 객체 배열을 출력하는 코드로 작성했습니다.</p>\n<deckgo-highlight-code language=\"javascript\"  >\n          <code slot=\"code\">const cheerio = require(&quot;cheerio&quot;);\r\nconst axios = require(&quot;axios&quot;);\r\n\r\n(async () =&gt; {\r\n  //크롤링 대상 URL, axios의 get은 비동기 함수이므로 async-await을 사용한다.\r\n  const html = await axios.get(&quot;http://openinsider.com/insider-purchases-25k&quot;),\r\n    $ = cheerio.load(html.data);\r\n\r\n  const trElements = $(&quot;#tablewrapper &gt; table &gt; tbody &gt; tr&quot;);\r\n  const insiderTradeData = trElements\r\n    .map((index, tr) =&gt; ({\r\n      date: $(tr).find(&quot;td:nth-child(3) &gt; div&quot;).text(),\r\n      ticker: $(tr).find(&quot;td:nth-child(4) &gt; b &gt; a&quot;).text(),\r\n    }))\r\n    .toArray();\r\n  console.log(insiderTradeData);\r\n})();</code>\n        </deckgo-highlight-code>\n<br>\n<p><code>node index</code>로 실행해보면 결과는 다음과 같이 나오는 걸 확인할 수 있습니다.</p>\n<p><img src=\"https://user-images.githubusercontent.com/71566740/133531609-93363fba-e51e-47aa-b03a-5cad03bc1795.png\" alt=\"131484221-8eaa2b8f-749e-46d9-8efe-486e4630e963\"></p>\n<h2>마지막으로</h2>\n<ul>\n<li>위에서 사용했던 http 라이브러리나 parsing 라이브러리 둘 다 크롤링에 활용 가능한 도구이지 '크롤링만을 위한 도구'가 아닙니다.\r\n<br>해당 라이브러리가 어떤 역할을 하는지 직접 찾아보시는 것도 좋을 것 같습니다.</li>\n<li>크롤링은 꼭 개발자가 아니더라도 원하는 데이터를 자급자족할 수 있다는 점에서 활용도가 정말 높습니다.\r\n<br>위에서 다뤘던 예제는 기초적인 크롤링 방법만을 다룬 것이므로 실제 크롤링을 이용해 무언가를 하려면 대상의 URL 규칙성, 페이지의 구조 등을 직접 분석해보면서 코드를 작성하는 방법을 고민해 보셔야 합니다.</li>\n<li>크롤링은 사람이 직접 데이터를 수집하는 것보다 훨씬 빠른 속도로 서버에 다수에 요청을 보내서 데이터를 응답받기 때문에 크롤링 대상 서버에 문제를 발생시킬 수 있습니다.\r\n<br> 위와 같은 이유로 크롤링이 차단되어 있는 사이트들도 있으니 크롤링이 허용되어 있는 사이트인지 확인하는 것도 중요합니다.</li>\n<li>크롤링으로 처벌을 받은 판례가 있으니 실제 서비스를 위한 코드를 작성할 때는 해당 정보가 크롤링이 허용되는 정보인지 잘 확인해 보시고 사용하는 게 좋을 것 같습니다.</li>\n</ul>","frontmatter":{"emoji":"📢","title":"크롤링을 통한 데이터 수집","date":"2021-08-31","description":"Node.js에서 크롤링을 이용해 데이터를 수집해보겠습니다.","tag":["Javascript","Data"]},"fields":{"slug":"/nodejs-crawling/"},"id":"2a3b304d-fff2-5587-9a14-4f3a1864ba1d"},"allMarkdownRemark":{"nodes":[{"fields":{"slug":"/gititle-project/"},"frontmatter":{"description":"좋은 커밋 메시지를 작성하기 위한 프로젝트!","title":"Gititle 프로젝트"},"id":"59da09b0-cfe8-5387-8b2c-18728bc48eb5"},{"fields":{"slug":"/sass-compiler/"},"frontmatter":{"description":"non-Node.js 환경에서 Sass를 사용해 봅시다.","title":"Live Sass Compiler 사용하기"},"id":"cddd5b1b-eb1c-5a3a-bf59-e59f20abc909"},{"fields":{"slug":"/nodejs-crawling/"},"frontmatter":{"description":"Node.js에서 크롤링을 이용해 데이터를 수집해보겠습니다.","title":"크롤링을 통한 데이터 수집"},"id":"2a3b304d-fff2-5587-9a14-4f3a1864ba1d"},{"fields":{"slug":"/copilot-review/"},"frontmatter":{"description":"Github의 코딩 AI Copilot을 한 달간 사용해본 경험을 공유합니다.","title":"Copilot 사용 후기"},"id":"8a284f98-aa79-5a11-9f60-17da05b03882"},{"fields":{"slug":"/typescript-express/"},"frontmatter":{"description":"Typescript를 이용해 Express 코드를 작성해봅시다.","title":"Typescript + Express 웹서버 구축하기"},"id":"7de321b0-d894-5d5a-a7e1-058f7c309364"},{"fields":{"slug":"/blog-remake-review1/"},"frontmatter":{"description":"블로그를 다시 만들기 시작하면서 느낀점을 공유합니다.","title":"Next.js에서 Gatsby로 블로그 이사 중간 리뷰"},"id":"4cfa4a82-589b-5649-8fd0-5cbb6f48d40b"},{"fields":{"slug":"/good-commit-message/"},"frontmatter":{"description":"제가 사용하는 커밋 메시지 작성 규칙을 공유합니다.","title":"깃 커밋 메시지 컨벤션 (Git Commit Message Convention)"},"id":"98992a08-a70f-5f2e-b087-f3ddf6331f1d"}]}},"pageContext":{"slug":"/nodejs-crawling/"}},
    "staticQueryHashes": []}